# A total of 16 32GB GPUs were used for training.

trainer: forces


# dataset:
#   - src: /workspaces/ocp-public/data/s2ef/200k/train/
#     normalize_labels: True
#     target_mean: -0.7554450631141663
#     target_std: 2.887317180633545
#     grad_target_mean: 0.0
#     grad_target_std: 2.887317180633545
#   - src: /workspaces/ocp-public/data/s2ef/200k/train/

dataset:
  - src: /mnt/datasets/s2ef/2M/train/
    normalize_labels: True
    target_mean: -0.7554450631141663
    target_std: 2.887317180633545
    grad_target_mean: 0.0
    grad_target_std: 2.887317180633545
  - src: /mnt/datasets/s2ef/all/val_id/



logger: wandb

task:
    dataset: trajectory_lmdb
    description: "Regressing to energies and forces for DFT trajectories from OCP"
    type: regression
    metric: mae
    labels:
        - potential energy
    grad_input: atomic forces
    train_on_free_atoms: True
    eval_on_free_atoms: True

    torch_compile:
        enabled: False
        debug: False
        max_autotune: True
        profile: True
        dynamic_shapes: False

    padded_collater: False
    # val_subset: [1, 2]
    # val_subset: 1000

    overfit:
        samples: 1
        size: 1000

model:
    name: escn
    num_layers: 12
    max_neighbors: 20
    cutoff: 12.0
    sphere_channels: 128
    hidden_channels: 256
    lmax_list: [4]
    mmax_list: [2]
    num_sphere_samples: 128
    distance_function: "gaussian"
    regress_forces: True
    use_pbc: True
    basis_width_scalar: 2.0
    otf_graph: True

optim:
    batch_size: 1
    eval_batch_size: 1
    num_workers: 8
    lr_initial: 0.0008
    optimizer: AdamW
    optimizer_params: {"amsgrad": True}
    eval_every: 5000
    lr_gamma: 0.3
    lr_milestones: # epochs at which lr_initial <- lr_initial * lr_gamma
        - 145833
        - 187500
        - 229166
    warmup_steps: 100
    warmup_factor: 0.2
    max_epochs: 12
    force_coefficient: 100
    energy_coefficient: 2
    clip_grad_norm: 20
    ema_decay: 0.999
    loss_energy: mae
    loss_force: mae
